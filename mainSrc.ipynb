{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c23a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rrish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rrish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#load all the libraries\n",
    "import json \n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21239d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training and testing data\n",
    "df_train=pd.read_json('train_spider.json')\n",
    "df_test=pd.read_json('train_others.json')\n",
    "df_dev=pd.read_json('dev.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e35e85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert question column to lower case and store the value in a new column question_p\n",
    "df_train['question']=df_train['question'].str.lower()\n",
    "df_test['question']=df_test['question'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54ec29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations from 'question' column\n",
    "#data['question'] = data['question'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "def remove_punctuations(question):\n",
    "    for punctuation in string.punctuation:\n",
    "            question = question.replace(punctuation, '')\n",
    "    return question\n",
    "# Apply to the DF series\n",
    "df_train['question_p'] = df_train['question'].apply(remove_punctuations)\n",
    "df_test['question_p'] = df_test['question'].apply(remove_punctuations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a053900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the question_p column and store in a new column \"question_p_tok\"\n",
    "df_train['question_p_tok']=df_train['question_p']\n",
    "k=0\n",
    "for i in df_train['question_p']:\n",
    "    df_train['question_p_tok'][k]=word_tokenize(i)\n",
    "    k=k+1\n",
    "    \n",
    "df_test['question_p_tok']=df_test['question_p']\n",
    "k=0\n",
    "for i in df_test['question_p']:\n",
    "    df_test['question_p_tok'][k]=word_tokenize(i)\n",
    "    k=k+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91150cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing using word net lemmitizer\n",
    "\n",
    "##training data\n",
    "df_train['question_p_lem']=df_train['question_p_tok']\n",
    "df_train['question_p_lem_postag']=df_train['question_p_tok']\n",
    "k=0\n",
    "wnl = WordNetLemmatizer()\n",
    "for i in df_train['question_p_lem']:\n",
    "    df_train['question_p_lem'][k] = ' '.join([wnl.lemmatize(words) for words in i]) \n",
    "    k=k+1  \n",
    "#pos tagging\n",
    "k=0\n",
    "for i in df_train['question_p_lem']:\n",
    "    df_train['question_p_tok'][k]=word_tokenize(i) #tokenize the lemmatized text and then pos tag it\n",
    "    df_train['question_p_lem_postag'][k] = nltk.pos_tag(df_train['question_p_tok'][k])\n",
    "    k=k+1\n",
    "    \n",
    "## testing data\n",
    "df_test['question_p_lem']=df_test['question_p_tok']\n",
    "df_test['question_p_lem_postag']=df_test['question_p_tok']\n",
    "k=0\n",
    "wnl = WordNetLemmatizer()\n",
    "for i in df_test['question_p_lem']:\n",
    "    df_test['question_p_lem'][k] = ' '.join([wnl.lemmatize(words) for words in i]) \n",
    "    k=k+1   \n",
    "#pos tagging \n",
    "k=0\n",
    "for i in df_test['question_p_lem']:\n",
    "    df_test['question_p_tok'][k]=word_tokenize(i) #tokenize the lemmatized text and then pos tag it\n",
    "    df_test['question_p_lem_postag'][k] = nltk.pos_tag(df_test['question_p_tok'][k])\n",
    "    k=k+1\n",
    "    \n",
    "df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c358b886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
